{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea 16 del curso CM-072\n",
    "\n",
    "* Nombre y apellidos:\n",
    "* Fecha de presentación: 13 de diciembre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de características: vectorizador\n",
    "\n",
    "Para algunos tipos de datos, por ejemplo, datos de texto, se debe aplicar un paso de extracción de características para convertirlos en características numéricas. Para ilustrar esto usamos el conjunto de datos de spam de SMS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifica el path\n",
    "import os\n",
    "# print(os.getcwd())\n",
    "with open(os.path.join(\"Tarea16\", \"SMSSpamCollection\")) as f:\n",
    "    lineas = [linea.strip().split(\"\\t\") for linea in f.readlines()]\n",
    "texto = [x[1] for x in lineas]\n",
    "y = [x[0] == \"ham\" for x in lineas]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "texto_entrenamiento, texto_prueba, y_entrenamiento, y_prueba = train_test_split(texto, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos la extracción de características manualmente, así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9641319942611191"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(texto_entrenamiento)\n",
    "\n",
    "X_entrenamiento = vectorizer.transform(texto_entrenamiento)\n",
    "X_prueba = vectorizer.transform(texto_prueba)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_entrenamiento, y_entrenamiento)\n",
    "clf.score(X_prueba, y_prueba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La situación en la que aprendemos una transformación y luego la aplicamos a los datos de prueba es muy común en el machine learning. Por lo tanto, scikit-learn tiene un atajo para esto `pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9641319942611191"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
    "pipeline.fit(texto_entrenamiento, y_entrenamiento)\n",
    "pipeline.score(texto_prueba, y_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código es mucho más corto y fácil de manejar. La construcción de piepelines no solo simplifica el código, sino que también es importante para la selección del modelo. Si queremos hacer una búsqueda grid C para ajustar al modelo de Regresión logística, hacemos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.1, 1, 10, 100]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(texto_entrenamiento)\n",
    "\n",
    "X_entrenamiento = vectorizer.transform(texto_entrenamiento)\n",
    "X_prueba = vectorizer.transform(texto_prueba)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "grid = GridSearchCV(clf, param_grid={'C': [.1, 1, 10, 100]}, cv=5)\n",
    "grid.fit(X_entrenamiento, y_entrenamiento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 . Puedes encontrar el error que se encuentra aquí. Esto se denomina \"contaminación\" del conjunto de pruebas y conduce a estimaciones demasiado optimistas del rendimiento de  generalización, o parámetros mal seleccionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con pipeline también se puede buscar parámetros de extracción de características con `GridSearchCV`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 100, 'tfidfvectorizer__ngram_range': (1, 2)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9863701578192252"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
    "\n",
    "params = {'logisticregression__C': [.1, 1, 10, 100],\n",
    "          \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (2, 2)]}\n",
    "grid = GridSearchCV(pipeline, param_grid=params, cv=5)\n",
    "grid.fit(texto_entrenamiento, y_entrenamiento)\n",
    "print(grid.best_params_)\n",
    "grid.score(texto_prueba, y_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 .Cree un pipeline a una regresión  Ridge y `StandardScaler` en el conjunto de datos de Boston ( `sklearn.datasets.load_boston`). Agrega el transformador `sklearn.preprocessing.PolynomialFeatures` como segundo paso de preprocesamiento y una búsqueda grid del grado de los polinomios (prueba `1`, `2` y `3`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu solucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
