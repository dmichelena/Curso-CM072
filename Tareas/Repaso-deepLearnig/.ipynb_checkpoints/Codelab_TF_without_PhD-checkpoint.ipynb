{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "### Introducción\n",
    "\n",
    "En este codelab, se aprenderá a cómo construir y entrenar una red neuronal que reconozca los dígitos escritos a mano. En el camino, a medida que mejora su red neuronal para lograr el 99% de precisión, también descubrirá las herramientas de la profesión que los profesionales del Deep learning utilizan para entrenar sus modelos de manera eficiente.\n",
    "\n",
    "Este codelab utiliza el conjunto de datos MNIST, una colección de 60,000 dígitos etiquetados que han mantenido ocupadas generaciones de PhD durante casi dos décadas. Resolverás el problema con menos de 100 líneas de código Python y TensorFlow.\n",
    "\n",
    "Lo que aprenderás\n",
    "\n",
    "   \n",
    "* Qué es una red neuronal y cómo entrenarla\n",
    "* Cómo construir una red neuronal básica de 1 capa con TensorFlow\n",
    "* Cómo agregar más capas\n",
    "* Consejos y trucos de entrenamiento: overfitting, dropout, learning rate decay …\n",
    "* Cómo solucionar problemas de redes neuronales profunda\n",
    "* Cómo construir redes convolucionales\n",
    "\n",
    "\n",
    "Lo que necesitarás\n",
    "\n",
    "   \n",
    "* Python 2 o 3 (se recomienda Python 3\n",
    "* Tensor Flow\n",
    "* Matplotlib (biblioteca de visualización de Python)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación: Intalación de Tensor Flow y obtención del código de muestra\n",
    "\n",
    "Instala el software necesario en tu computadora: Python, TensorFlow y Matplotlib. Clonamos el repositorio de Github:\n",
    "\n",
    "```git clone https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd.git\n",
    "cd tensorflow-without-a-phd/tensorflow-mnist-tutorial\n",
    "```\n",
    "\n",
    "![](tf1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código de muestra para este tutorial se encuentra en la carpeta `tensorflow-mnist-tutorial`. La carpeta contiene múltiples archivos. El único en el que trabajará es `mnist_1.0_softmax.py`. Otros archivos son soluciones o código de soporte para cargar los datos y visualizar los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se ejecuta el script de python inicial, se ve  una visualización en tiempo real del proceso de entrenamiento:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tf2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar el programa se obtiene:\n",
    "\n",
    "![](tf3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar una red neuronal\n",
    "\n",
    "Primero veremos una red neuronal que está siendo entrenada.  Nuestra red neuronal toma los dígitos escritos a mano y los clasifica, es decir, si los reconoce como un 0, un 1, un 2 y así sucesivamente hasta un 9. Lo hace en función de las variables internas (\"pesos\" y \"sesgos\") que necesitan tener un valor correcto para que la clasificación funcione bien. Este \"valor correcto\" se aprende a través de un proceso de entrenamiento. Lo que necesita saber por ahora es que el ciclo de entrenamiento se ve así:\n",
    "\n",
    "```\n",
    "Dígitos de entrenamiento => actualizaciones de pesos y sesgos => mejor reconocimiento (bucle)\n",
    "```\n",
    "\n",
    "Repasemos los seis paneles de la visualización uno por uno para ver qué se necesita para entrenar una red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tf4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se puede ver los dígitos de entrenamiento alimentados en el ciclo  de entrenamiento, 100 a la vez. También se observa si la red neuronal, en su estado actual de entrenamiento, los ha reconocido (fondo blanco) o los ha clasificado erróneamente (fondo rojo con la etiqueta correcta en letra pequeña en el lado izquierdo y una  etiqueta erróneamente calculada a la derecha de cada dígito )\n",
    "\n",
    "Hay 50,000 dígitos de entrenamiento en este conjunto de datos. Suministramos 100 de ellos en el ciclo de entrenamiento en cada iteración, por lo que el sistema habrá visto todos los dígitos de entrenamiento una vez después de 500 iteraciones. Llamamos a esto una \"epoch\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tf5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar la calidad del reconocimiento en condiciones reales, debemos usar dígitos que el sistema NO haya visto durante el entrenamiento. De lo contrario, podría aprender todos los dígitos del entrenamiento de memoria y aún fallar al reconocer un nuevo  \"8\" que se pueda  de escribir. \n",
    "\n",
    "El conjunto de datos MNIST contiene 10.000 dígitos de prueba. Aquí puede ver alrededor de 1000 de ellos con todos los mal reconocidos ordenados en la parte superior (sobre un fondo rojo). La escala de la izquierda le da una idea aproximada de la precisión del clasificador (% de dígitos de prueba correctamente reconocidos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tf6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para desarrollar el entrenamiento, definiremos una función de pérdida, es decir, un valor que representa qué tan mal el sistema reconoce los dígitos y trata de minimizarlo. Lo que se ve aquí es que la pérdida disminuye tanto en el entrenamiento como en los datos de prueba a medida que avanza el entrenamiento : eso es bueno. Significa que la red neuronal está aprendiendo. El eje X representa iteraciones a través del ciclo de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tf7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La precisión es simplemente el % de dígitos correctamente reconocidos. Esto se calcula tanto en el entrenamiento como en el conjunto de prueba. Se vera que crece si el entrenamiento va bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los dos últimos gráficos representan la dispersión de todos los valores tomados por las variables internas, es decir, los pesos y los sesgos a medida que avanza el entrenamiento. Aquí se  puede ver, por ejemplo, que los sesgos comenzaron inicialmente en 0 y terminaron tomando valores repartidos aproximadamente de manera uniforme entre -1.5 y 1.5. Estos gráficos pueden ser útiles si el sistema no converge bien. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tf8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuestas**: Los pesos (weigth) son parámetros numéricos que determinan con qué intensidad cada una de las neuronas afecta a la otra. Para una neurona típica, si las entradas son $x_1, x_2$ y $x_3$, los pesos  que se les aplicarán se denominan $w_1, w_2$ y $w_3$. Luego la salida es:\n",
    "$$\n",
    "y = f(x) = \\sum_{i=1}^{n} X_iW_i\n",
    "$$\n",
    "\n",
    "donde `i` varía desde  1 al número  de entradas.\n",
    "\n",
    "El sesgo(bias) es como la intersección agregada en una ecuación lineal. Es un parámetro adicional que se utiliza para ajustar la salida junto con la suma ponderada de las entradas a la neurona.\n",
    "El procesamiento realizado por una neurona se denota así:\n",
    "\n",
    "```\n",
    "salida = suma (peso * entradas) + sesgo\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal de una capa\n",
    "\n",
    "Los dígitos escritos en el conjunto de datos MNIST son imágenes de escala de grises de 28x28 píxeles. El enfoque más simple para clasificarlos es usar los 28x28 = 784 píxeles como entradas para una red neuronal de 1 capa.\n",
    "\n",
    "![](tf9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada \"neurona\" en una red neuronal hace una suma ponderada de todas sus entradas, agrega una constante llamada \"sesgo\" y luego alimenta el resultado a través de alguna función de activación no lineal.\n",
    "\n",
    "Aquí diseñamos una red neuronal de 1 capa con 10 neuronas de salida, ya que queremos clasificar los dígitos en 10 clases (0 a 9).\n",
    "\n",
    "Para un problema de clasificación, una función de activación que funciona bien es **softmax**. La aplicación de softmax en un vector se realiza tomando el exponencial de cada elemento y luego normalizando el vector (usando cualquier norma, como la longitud euclidiana de un vector).\n",
    "\n",
    "![](tf10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué se llama softmax? La exponencial es una función que aumenta abruptamente . Aumentará las diferencias entre los elementos del vector. También produce rápidamente grandes valores. Luego, a medida que se  normaliza el vector, el elemento más grande, que domina la norma, se normalizará a un valor cercano a 1, mientras que todos los otros elementos terminarán divididos por un valor grande y se normalizarán a algo cercano a 0. El vector resultante muestra claramente cuál fue su elemento más grande, el \"máximo\", pero conserva el orden relativo original de sus valores, de ahí lo de  \"soft\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, resumiremos el comportamiento de esta capa única de neuronas en una fórmula simple utilizando una multiplicación de matriz. Hagámoslo directamente para un \"mini-lote\" de 100 imágenes como entrada, produciendo 100 predicciones (vectores de 10 elementos) como salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tf11.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la primera columna de pesos en la matriz de pesos W, calculamos la suma de pesos de todos los píxeles de la primera imagen. Esta suma corresponde a la primera neurona. Usando la segunda columna de pesos, hacemos lo mismo para la segunda neurona y así sucesivamente hasta la décima neurona. Entonces podemos repetir la operación para las 99 imágenes restantes. Si llamamos a X la matriz que contiene nuestras 100 imágenes, todas las sumas ponderadas de nuestras 10 neuronas, calculadas en 100 imágenes son simplemente X.W (multiplicación de la matriz).\n",
    "\n",
    "Cada neurona ahora debe agregar su sesgo (una constante). Como tenemos 10 neuronas, tenemos 10 constantes de polarización. Llamaremos a este vector de 10 valores b. Debe agregarse a cada línea de la matriz calculada previamente. Usando un poco de magia llamada \"broadcasting\" escribiremos esto con un signo más simple.\n",
    "\n",
    "\"Broadcasting\" es un truco estándar utilizado en Python y numpy. Extiende cómo funcionan las operaciones normales en matrices con dimensiones incompatibles. \"Broadcasting\" significa  que \"si está agregando dos matrices pero no puede hacerlo porque sus dimensiones no son compatibles, intenta replicar la de menor dimensión  tanto como sea necesario para que funcione\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente aplicamos la función de activación de softmax y obtenemos la fórmula que describe una red neuronal de 1 capa, aplicada a 100 imágenes:\n",
    "\n",
    "![](tf11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradiente de descenso\n",
    "\n",
    "Ahora que nuestra red neuronal produce predicciones a partir de imágenes de entrada, necesitamos medir qué tan buenas son, es decir, la distancia entre lo que la red nos dice y lo que sabemos que es verdad. Se ha de señalar que tenemos etiquetas verdaderas para todas las imágenes en este conjunto de datos.\n",
    "\n",
    "Cualquier distancia funcionaría, la distancia euclidiana ordinaria es buena, pero para los problemas de clasificación una distancia, llamada \"entropía cruzada\" es más eficiente.\n",
    "\n",
    "![](tf12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La codificación \"one-hot\" significa que se representa la etiqueta \"6\" usando un vector de 10 valores, de todos ceros pero el 6to valor con un valor de 1. Es útil aquí porque el formato es muy similar a cómo nuestra red neuronal genera predicciones, también como un vector de 10 valores.\n",
    "\n",
    "\"Entrenar\" a la red neuronal en realidad significa usar imágenes y etiquetas de entrenamiento para ajustar los pesos y los sesgos a fin de minimizar la función de pérdida de entropía cruzada. Así es como funciona.\n",
    "\n",
    "La entropía cruzada es una función de los pesos, los sesgos, los píxeles de la imagen de entrenamiento y su etiqueta conocida.\n",
    "\n",
    "Si calculamos las derivadas parciales de la entropía cruzada con relación a todos los pesos y todos los sesgos obtenemos un \"gradiente\", calculado para una imagen, etiqueta y valor presente de pesos y sesgos dados. Hay que recordar que tenemos 7850 pesos y sesgos, por lo que calcular el gradiente suena como un montón de trabajo. Afortunadamente, TensorFlow lo hará por nosotros.\n",
    "\n",
    "La propiedad matemática de un gradiente es que señala en qué dirección te tienes que mover para incrementar el valor de de una función lo más rápido posible. Como queremos ir donde la entropía cruzada es baja, vamos en la dirección opuesta. Actualizamos los pesos y los sesgos por una fracción del gradiente y hacemos lo mismo otra vez usando el siguiente lote de imágenes de entrenamiento. Con suerte, esto nos lleva al fondo del pozo donde la entropía cruzada es mínima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tf13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta imagen, la entropía cruzada se representa como una función de 2 pesos. En realidad, hay muchos más. El algoritmo de descenso de gradiente sigue el camino de descenso más pronunciado hacia un mínimo local. Las imágenes de entrenamiento se cambian en cada iteración para que converger  hacia un mínimo local que funcione para todas las imágenes.\n",
    "\n",
    "\"Tasa de aprendizaje\": no puede actualizar sus pesos y sesgos por toda la longitud del gradiente en cada iteración. Para llegar al final, se deben realizar pasos  pequeños, es decir, usar solo una fracción del gradiente, llamada  \"tasa de aprendizaje\".\n",
    "\n",
    "Para resumir, así es como se ve el ciclo de entrenamiento:\n",
    "\n",
    "```\n",
    "Dígitos y etiquetas de entrenamiento => función de pérdida => gradiente => descenso más pronunciado => actualizamos pesos y sesgos => repetir con el siguiente mini-lote de imágenes y etiquetas de entrenamiento\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué trabajar con \"mini-lotes\" de 100 imágenes y etiquetas?\n",
    "\n",
    "Definitivamente  se puede calcular el gradiente en una sola imagen de ejemplo y actualizar los pesos y los sesgos inmediatamente (\"descenso de gradiente estocástico\" ). Hacerlo en 100 ejemplos da un gradiente que representa mejor las restricciones impuestas por diferentes imágenes de ejemplo y por lo tanto, es probable que converja hacia la solución más rápidamente. Sin embargo, el tamaño del mini-lote es un parámetro ajustable. \n",
    "\n",
    "Hay otra razón más técnica: trabajar con lotes también significa trabajar con matrices más grandes y estas suelen ser más fáciles de optimizar en GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código para red neuronal de una capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNISTdata\\train-images-idx3-ubyte.gz\n",
      "Extracting MNISTdata\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNISTdata\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNISTdata\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "\n",
    "\n",
    "NUM_ITERS=5000\n",
    "LOTES=100\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "mnist = read_data_sets(\"MNISTdata\", one_hot=True, reshape=False, validation_size=0)\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "W = tf.Variable(tf.truncated_normal([784, 10],stddev=0.1))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "\n",
    "XX = tf.reshape(X, [-1, 784])\n",
    "\n",
    "# Definimos el modelo\n",
    "Y = tf.nn.softmax(tf.matmul(XX, W) + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos que cada imagen en MNIST es de un dígito escrito a mano entre cero y nueve. Entonces, solo hay diez cosas posibles que una imagen determinada puede ser. Queremos poder mirar una imagen y dar las probabilidades de que sea un  dígito. \n",
    "Si desea asignar probabilidades a un objeto siendo una de varias cosas diferentes, softmax es adecuado para este problema, ya que  softmax nos da una lista de valores entre 0 y 1 que suman 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos la entropia cruzada\n",
    "\n",
    "entropia_cruzada = -tf.reduce_mean(Y_ * tf.log(Y)) * 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_correcta = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "prediccion = tf.reduce_mean(tf.cast(prediccion_correcta, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que TensorFlow conoce el gráfico completo de los cálculos, puede utilizar automáticamente el algoritmo de retropropagación para determinar de manera eficiente cómo afectan sus variables a la pérdida que le pide que minimice. Luego se puede aplicar un algoritmo de optimización para modificar las variables y reducir la pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paso_entrenamiento = tf.train.GradientDescentOptimizer(0.003).minimize(entropia_cruzada)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, le pedimos a TensorFlow que minimice `entropia cruzada` usando el algoritmo de descenso de gradiente con una tasa de aprendizaje de 0.003. El descenso de gradiente es un procedimiento simple, donde TensorFlow simplemente cambia cada variable un poco en la dirección que reduce el costo. \n",
    "\n",
    "Lo que hace TensorFlow  es agregar nuevas operaciones al grafo que implementa la retropropagación y el descenso del gradiente. Luego devuelve una sola operación que, cuando se ejecuta, da un paso de entrenamiento de descenso de gradiente, ajustando ligeramente las variables para reducir la pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancemos el modelo utilizando InteractiveSession:\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_ITERS + 1):\n",
    "    lote_X, lote_Y = mnist.train.next_batch(LOTES)\n",
    "    sess.run(paso_entrenamiento, feed_dict={X: lote_X, Y_:lote_Y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar el modelo, primero hay que  predecir la etiqueta correcta. `tf.argmax` es una función que  proporciona el índice de la entrada más alta en un tensor a lo largo de algún eje. Por ejemplo, `tf.argmax(Y, 1)` es la etiqueta que el modelo propuesto predice que es más probable para cada entrada, mientras que `tf.argmax (y_, 1)` es la etiqueta correcta. Podemos usar `tf.equal` para verificar si nuestra predicción coincide con la verdadera.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_correcta = tf.equal(tf.argmax(Y,1), tf.argmax(Y_,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eso nos da una lista de booleanos. Para determinar qué fracción es la correcta, seleccionamos los números de coma flotante y luego tomamos la media. Por ejemplo, `[True, False, True, True]` se convertiría en `[1,0,1,1]` que se convertiría en 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = tf.reduce_mean(tf.cast(prediccion_correcta, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, calculamos precisión de nuestros datos de  prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9232\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(prediccion, feed_dict={X: mnist.test.images, Y_: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
